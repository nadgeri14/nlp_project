{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eb21d42-02ad-41ed-9130-96c26242cb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Psycopg2 not installed\n",
      "Ssh tunnel not installed\n",
      "Found no/flawed database config. This is only a problem if you want to use the dasp database!\n"
     ]
    }
   ],
   "source": [
    "from data_collection.reddit_user_dataset import RedditUserDataset\n",
    "from classification.feature_computing import Embedder\n",
    "from utils.file_sort import path_sort\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import pickle as pkl\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from argparse import ArgumentParser\n",
    "import argparse\n",
    "import gzip\n",
    "import sys\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2b0538-3993-4951-a97e-e348f60a4ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_source_graphs = False\n",
    "source_threshold = 0.8\n",
    "embed_mode = 'avg'\n",
    "start_date = datetime.date(2020, 1, 1)\n",
    "end_date = datetime.date(2021, 4, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cefad0f-02ca-4dfa-a201-05f1f6395663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate timeframes\n",
    "delta_days = 30\n",
    "offset_days = 30\n",
    "curr_date = start_date\n",
    "timeframes = []\n",
    "while curr_date + datetime.timedelta(days=delta_days) < end_date:\n",
    "    print(curr_date)\n",
    "    timeframes.append((curr_date, curr_date + datetime.timedelta(days=delta_days)))\n",
    "    curr_date = curr_date + datetime.timedelta(days=offset_days)\n",
    "\n",
    "print(timeframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcce6321-08a9-44e8-a3bb-d9d84e77b5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset_path = '../data/datasets/reddit_corpus_final_with_fn_scale.gzip'\n",
    "base_dataset = RedditUserDataset.load_from_file(base_dataset_path, compression='gzip')\n",
    "doc_embedding_file_header = 'usr2vec'\n",
    "\n",
    "headers = base_dataset.data_frame.columns\n",
    "doc_embedding_file_path = '../data/embeddings/usr2vec'\n",
    "\n",
    "if doc_embedding_file_header not in headers:\n",
    "    print('The selected embedding header is not in the dataframe. Trying to add it from the given path...')\n",
    "    try:\n",
    "        base_dataset.add_embedding_file_column(doc_embedding_file_header, doc_embedding_file_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Tried to add embedding header automatically, but failed. Exiting...')\n",
    "        sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e0d0b7-8b32-44c3-88f6-20c58cf4a718",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset.store_to_file(base_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f976c841-9ea2-4c2c-8dc6-ea8b3bd19c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_source_graphs:\n",
    "    print(\"Generating timeframed embeddings\")\n",
    "    if args.graph_type == 'social':\n",
    "        base_dataset.cache_social_graph(args.user_interaction_file_dir)\n",
    "    for index, tf in tqdm(enumerate(timeframes)):\n",
    "        print(\"Generating source timeframe...\")\n",
    "        start = time.time()\n",
    "        if args.graph_type == 'linguistic':\n",
    "            framed = base_dataset.timeframed_documents(tf, inplace=False)\n",
    "            framed.generate_similarity_triplet_list(args.doc_embedding_file_path, args.doc_embedding_file_header, source_threshold, embed_mode,\n",
    "                                                    similarity_metric=args.similarity_metric)\n",
    "        elif args.graph_type == 'social':\n",
    "            # Format ('2020-09-01', '2020-09-30')\n",
    "            formatted_timeframe = (tf[0], tf[1])\n",
    "            print(formatted_timeframe)\n",
    "            framed = base_dataset.load_social_graph_from_cache(formatted_timeframe, inplace=False)\n",
    "            framed.timeframed_documents(tf, inplace=True)\n",
    "        else:\n",
    "            raise Exception(\"Invalid graph type\")\n",
    "        framed.store_instance_to_file(source_graph_path + 'source_graph_' + str(index) + '.pkl')\n",
    "        end = time.time()\n",
    "        print(\"Elapsed time:\" + str(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e08a19-16f7-4898-961b-4cbc8bf9ae07",
   "metadata": {},
   "source": [
    "# Embedder test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8285e22-35f7-4583-a996-607cff260851",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c24518e359f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/embeddings/bert_embeddings/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/temporal-misinformation-spreaders/src/classification/feature_computing.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embeddings_dir, embeddings_file)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                     \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fake_news_project/lib/python3.9/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "embedder = Embedder('../data/embeddings/bert_embeddings/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ac659f-3f83-4c18-95a5-124ed673ff6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df3374-98a2-437c-867a-973a5033eeee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

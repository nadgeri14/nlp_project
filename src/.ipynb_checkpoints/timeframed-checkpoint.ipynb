{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac990e91-68c1-40d7-9c26-a480a48f3ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_collection.reddit_user_dataset import RedditUserDataset\n",
    "from classification.feature_computing import Embedder\n",
    "from utils.file_sort import path_sort\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import pickle as pkl\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from argparse import ArgumentParser\n",
    "import argparse\n",
    "import gzip\n",
    "import sys\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a76a4490-9d2e-403a-ac5d-693ae5113306",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.date(2020, 1, 1)\n",
    "end_date = datetime.date(2021, 4, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5ddb25d-85bb-40a1-840e-0df32b1713af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01\n",
      "2020-01-31\n",
      "2020-03-01\n",
      "2020-03-31\n",
      "2020-04-30\n",
      "2020-05-30\n",
      "2020-06-29\n",
      "2020-07-29\n",
      "2020-08-28\n",
      "2020-09-27\n",
      "2020-10-27\n",
      "2020-11-26\n",
      "2020-12-26\n",
      "2021-01-25\n",
      "2021-02-24\n",
      "2021-03-26\n"
     ]
    }
   ],
   "source": [
    "# Generate timeframes\n",
    "delta_days = 30\n",
    "offset_days = 30\n",
    "curr_date = start_date\n",
    "timeframes = []\n",
    "while curr_date + datetime.timedelta(days=delta_days) < end_date:\n",
    "    print(curr_date)\n",
    "    timeframes.append((curr_date, curr_date + datetime.timedelta(days=delta_days)))\n",
    "    curr_date = curr_date + datetime.timedelta(days=offset_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0cd8264-7c9a-474e-a75e-1f7f0fc04304",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset_path = '../data/reddit_dataset/reddit_corpus_unbalanced_filtered.gzip'\n",
    "base_dataset = RedditUserDataset.load_from_file(base_dataset_path, compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfb3b7bf-aadb-4ac2-bb92-444122690e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_filepath = ['../data/embeddings/usr2vec/']\n",
    "dim = 200\n",
    "embed_type = 'usr2vec'\n",
    "embedder = Embedder(embeddings_filepath, embed_type, dim=dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ea294bd-91cd-44ec-821f-4c410b5a54f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating source timeframe...\n",
      "Exception while embedding user f4c297d70c985a246ce50bb9c2554d0e2a7743e1488dcf07bed4b4e990de342a\n",
      "'f4c297d70c985a246ce50bb9c2554d0e2a7743e1488dcf07bed4b4e990de342a'\n",
      "Exception while embedding user 638b517f673fe4e9cf6bb08405812c3f423fa8dceef0f273fa1e5260bb8652e4\n",
      "'638b517f673fe4e9cf6bb08405812c3f423fa8dceef0f273fa1e5260bb8652e4'\n",
      "Exception while embedding user b2e4450e64e3e19ef5a4dea085959c572f22fa4fa2f2c6b7d3f7c47aeb1c7ee4\n",
      "'b2e4450e64e3e19ef5a4dea085959c572f22fa4fa2f2c6b7d3f7c47aeb1c7ee4'\n",
      "Exception while embedding user 285d52d8050a872b6c95e6b2971586f312765ae76fe8047a2f3554ab75d76bed\n",
      "'285d52d8050a872b6c95e6b2971586f312765ae76fe8047a2f3554ab75d76bed'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [06:19, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'source_graph_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4e2b60a5bd03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid graph type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mframed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_instance_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_graph_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'source_graph_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Elapsed time:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'source_graph_path' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "graph_type = 'linguistic'\n",
    "source_threshold = 0.75\n",
    "embed_mode = 'avg'\n",
    "similarity_metric = 'cosine_similarity'\n",
    "\n",
    "for time_index, tf in tqdm(enumerate(timeframes)):\n",
    "    print(\"Generating source timeframe...\")\n",
    "    start = time.time()\n",
    "    if graph_type == 'linguistic':\n",
    "        #framed = base_dataset.timeframed_documents(tf, inplace=False)\n",
    "        framed = RedditUserDataset(base_dataset.data_frame.drop(columns=['documents', 'embedding_file', 'annotation', 'bias_counter', 'factual_counter']))\n",
    "        framed.generate_similarity_triplet_list(embedder, source_threshold, embed_mode,time_index,\n",
    "                                                similarity_metric=similarity_metric)\n",
    "    elif graph_type == 'social':\n",
    "        # Format ('2020-09-01', '2020-09-30')\n",
    "        formatted_timeframe = (tf[0], tf[1])\n",
    "        print(formatted_timeframe)\n",
    "        framed = base_dataset.load_social_graph_from_cache(formatted_timeframe, inplace=False)\n",
    "        framed = RedditUserDataset(base_dataset.data_frame.drop(columns=['documents', 'embedding_file', 'annotation', 'bias_counter', 'factual_counter']))\n",
    "    else:\n",
    "        raise Exception(\"Invalid graph type\")\n",
    "    #framed.store_instance_to_file(source_graph_path + 'source_graph_' + str(time_index) + '.pkl')\n",
    "    end = time.time()\n",
    "    print(\"Elapsed time:\" + str(end - start))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "221e2f0b-6936-4998-afe8-571e3e5d9f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sim_triple in framed.similarity_triplets:\n",
    "    sim = sim_triple[-1]\n",
    "    if sim < 0.75:\n",
    "        print(sim_triple)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e658763-b9f1-4808-a070-1a634cbdbf4e",
   "metadata": {},
   "source": [
    "# Bert Linguistic Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc8a4c84-4db5-4d25-ad2f-6886fc80d860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/home/plepi/temporal-misinformation-spreaders/src/classification/feature_computing.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  user_embedding.append(torch.tensor(embedding))\n"
     ]
    }
   ],
   "source": [
    "embeddings_filepath = ['../data/embeddings/bert/']\n",
    "dim = 768\n",
    "embed_type = 'bert'\n",
    "embedder = Embedder(embeddings_filepath, embed_type, dim=dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec6229f7-74c9-4165-982e-ff22b304f7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating source timeframe...\n",
      "Exception while embedding user b2e4450e64e3e19ef5a4dea085959c572f22fa4fa2f2c6b7d3f7c47aeb1c7ee4\n",
      "'b2e4450e64e3e19ef5a4dea085959c572f22fa4fa2f2c6b7d3f7c47aeb1c7ee4'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [06:25, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time:385.6063551902771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "graph_type = 'linguistic'\n",
    "source_threshold = 0.75\n",
    "embed_mode = 'avg'\n",
    "similarity_metric = 'cosine_similarity'\n",
    "\n",
    "for time_index, tf in tqdm(enumerate(timeframes)):\n",
    "    print(\"Generating source timeframe...\")\n",
    "    start = time.time()\n",
    "    if graph_type == 'linguistic':\n",
    "        #framed = base_dataset.timeframed_documents(tf, inplace=False)\n",
    "        framed_bert = RedditUserDataset(base_dataset.data_frame.drop(columns=['documents', 'embedding_file', 'annotation', 'bias_counter', 'factual_counter']))\n",
    "        framed_bert.generate_similarity_triplet_list(embedder, source_threshold, embed_mode,time_index,\n",
    "                                                similarity_metric=similarity_metric)\n",
    "    elif graph_type == 'social':\n",
    "        # Format ('2020-09-01', '2020-09-30')\n",
    "        formatted_timeframe = (tf[0], tf[1])\n",
    "        print(formatted_timeframe)\n",
    "        framed_bert = base_dataset.load_social_graph_from_cache(formatted_timeframe, inplace=False)\n",
    "        framed_bert = RedditUserDataset(base_dataset.data_frame.drop(columns=['documents', 'embedding_file', 'annotation', 'bias_counter', 'factual_counter']))\n",
    "    else:\n",
    "        raise Exception(\"Invalid graph type\")\n",
    "    #framed.store_instance_to_file(source_graph_path + 'source_graph_' + str(time_index) + '.pkl')\n",
    "    end = time.time()\n",
    "    print(\"Elapsed time:\" + str(end - start))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b385b7fb-12b6-4136-8227-036b4fbdd74d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

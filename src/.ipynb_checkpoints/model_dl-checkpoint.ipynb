{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3331f9f-da6c-4831-a731-da92abbcb809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Psycopg2 not installed\n",
      "Ssh tunnel not installed\n",
      "Found no/flawed database config. This is only a problem if you want to use the dasp database!\n"
     ]
    }
   ],
   "source": [
    "from data_collection.reddit_user_dataset import RedditUserDataset\n",
    "from data_collection.reddit_user_dataset import convert_timeframes_to_model_input\n",
    "from classification.feature_computing import Embedder\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import datetime\n",
    "import random\n",
    "import gzip\n",
    "import time\n",
    "import os\n",
    "from os import listdir\n",
    "from utils.file_sort import path_sort\n",
    "from argparse import ArgumentParser\n",
    "import argparse\n",
    "import numpy as np\n",
    "from os.path import isfile, join\n",
    "from random import randrange\n",
    "import json\n",
    "from utils.utils import *\n",
    "import torch\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8034b6eb-1880-4125-8c62-e5ea194c6b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_type = 'usr2vec'\n",
    "users_embeddings = {}\n",
    "embeddings_dir = ['../data/embeddings/usr2vec']\n",
    "dim = 200\n",
    "\n",
    "if 'usr2vec' in embeddings_type:\n",
    "    files = glob.glob(os.path.join(embeddings_dir[0], '*.txt'))\n",
    "    files = sorted(files, key= lambda file: int(file.split('.')[-2].split('_')[-1]))\n",
    "\n",
    "    for index, file in enumerate(files):\n",
    "        with open(file) as f:\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                values = line.split(' ')\n",
    "                user_id = values[0]\n",
    "                embedding = np.array(values[-200:]).astype(np.double)\n",
    "                user_embedding = users_embeddings.get(user_id, [])\n",
    "                if len(user_embedding) < index and len(user_embedding) > 0:\n",
    "                    current = user_embedding[-1]\n",
    "                elif len(user_embedding) < index and len(user_embedding) == 0:\n",
    "                    current = torch.rand(dim)\n",
    "                    \n",
    "                while len(user_embedding) < index:\n",
    "                    user_embedding.append(current)\n",
    "                    \n",
    "                user_embedding.append(torch.tensor(embedding))\n",
    "                users_embeddings[user_id] = user_embedding\n",
    "    \n",
    "    for user, values in users_embeddings.items():\n",
    "        while len(values) < 16:\n",
    "            values.append(torch.zeros(dim))\n",
    "\n",
    "    #users_embeddings[user] = [torch.stack(values).mean(axis=0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46a3e8a2-4cdf-4c79-a0e7-621eba70c527",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'users_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-11a87f98d6a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_users\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32min\u001b[0m \u001b[0musers_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'users_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "test_users = []\n",
    "for user, values in users_embeddings.items():\n",
    "    if torch.equal(values[-4].double(), torch.zeros(dim).double()) and torch.equal(values[-3].double(),torch.zeros(dim).double()) \\\n",
    "        and torch.equal(values[-2].double(), torch.zeros(dim).double()) and torch.equal(values[-1].double(), torch.zeros(dim).double()):\n",
    "        c += 1\n",
    "    else:\n",
    "        test_users.append(user)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87a1fcc7-1f17-499f-aa08-02adf4814ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(input_list):\n",
    "    norm_list = list()\n",
    "\n",
    "    if isinstance(input_list, list):\n",
    "        sum_list = sum(input_list)\n",
    "\n",
    "        for value in input_list:\n",
    "            tmp = value / sum_list\n",
    "            norm_list.append(tmp)\n",
    "\n",
    "    return norm_list\n",
    "\n",
    "\n",
    "def chunk_data(to_chunk, amount):\n",
    "    chunk_size = len(to_chunk) // amount\n",
    "    chunk_size = max(1, chunk_size)\n",
    "    return list((to_chunk[i:i + chunk_size] for i in range(0, len(to_chunk), chunk_size)))[:amount]\n",
    "\n",
    "def fold_data(chunked: [[]], train_split: int):\n",
    "    train_data = [idx for fold in chunked[0:train_split] for idx in fold]\n",
    "    val_data = [idx for fold in chunked[train_split:train_split + 1] for idx in fold]\n",
    "    test_data = [idx for fold in chunked[train_split + 1:] for idx in fold]\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def read_userids(filename, root='../data/user_splits_712'):\n",
    "    ids = None\n",
    "    with open(os.path.join(root, filename), 'r') as f: \n",
    "        ids = [str(line.strip()) for line in f]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4413dda-d17c-4a4a-a830-bb15902a0451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_dataset_path = '../data/reddit_dataset/reddit_corpus_unbalanced_filtered.gzip'\n",
    "base_dataset_path='../data/reddit_dataset/reddit_corpus_unbalanced_filtered.gzip' \n",
    "source_frame_dir='../data/reddit_dataset/social/usr2vec_delta30/source' \n",
    "target_dir='../data/reddit_dataset/model_samples_social/usr2vec_delta30_new/'\n",
    "base_dataset = RedditUserDataset.load_from_file(base_dataset_path, compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06897d88-2b22-46c1-9408-2a119f645323",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_user_split = False\n",
    "if do_user_split and (user_fold_index > user_fold_amount - 1 or user_fold_index < 0):\n",
    "    raise Exception(\"Invalid user_fold_index!\")\n",
    "\n",
    "# Build ground truth\n",
    "ground_truth = {}\n",
    "for index, row in base_dataset.data_frame.iterrows():\n",
    "    ground_truth[index] = row['fake_news_spreader']\n",
    "\n",
    "print(\"Running model dataloader with target dir {}\".format(target_dir))\n",
    "\n",
    "if not os.path.exists(os.path.join(target_dir, 'train_samples/')):\n",
    "    os.makedirs(os.path.join(target_dir, 'train_samples/'))\n",
    "if not os.path.exists(os.path.join(target_dir, 'val_samples/')):\n",
    "    os.makedirs(os.path.join(target_dir, 'val_samples/'))\n",
    "if not os.path.exists(os.path.join(target_dir, 'test_samples/')):\n",
    "    os.makedirs(os.path.join(target_dir, 'test_samples/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc0351ae-84ce-4cb5-9a91-026a5942e295",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-22ae2dca1a40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmerge_liwc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'false'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0membedder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_embedding_file_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtimeframed_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/temporal-misinformation-spreaders/src/classification/feature_computing.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embeddings_dir, embeddings_type, dim, read_personality)\u001b[0m\n\u001b[1;32m     63\u001b[0m                         \u001b[0muser_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                     \u001b[0muser_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musers_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "doc_embedding_file_path='../data/embeddings/bert/' \n",
    "embed_type='bert'\n",
    "merge_liwc='false'\n",
    "dim=768\n",
    "embedder = Embedder([doc_embedding_file_path], embed_type, dim)\n",
    "threshold = 0.8\n",
    "timeframed_dataset = []\n",
    "doc_amount_avgs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed570fb3-4705-4568-9090-27e6a15782f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dir = [doc_embedding_file_path]\n",
    "dim = 768\n",
    "users_embeddings  = {}\n",
    "files = glob.glob(os.path.join(embeddings_dir[0], '*.pkl'))\n",
    "files = sorted(files, key= lambda file: int(file.split('.')[-2].split('_')[-1]))\n",
    "\n",
    "for index, file in enumerate(files):\n",
    "    temp_embeddings = pkl.load(open(file, 'rb'))\n",
    "    for user_id, embedding in temp_embeddings.items():\n",
    "        user_embedding = users_embeddings.get(user_id, [])\n",
    "\n",
    "        if len(user_embedding) < index and len(user_embedding) > 0:\n",
    "            current = user_embedding[-1]\n",
    "        elif len(user_embedding) < index and len(user_embedding) == 0:\n",
    "            current = torch.rand(dim)\n",
    "\n",
    "        while len(user_embedding) < index:\n",
    "            user_embedding.append(current)\n",
    "\n",
    "        user_embedding.append(torch.tensor(embedding))\n",
    "        users_embeddings[user_id] = user_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed643b6-c62c-4755-b03b-39f22892edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "\n",
    "for graph in path_sort(\n",
    "        [join(source_frame_dir, f) for f in listdir(source_frame_dir) if isfile(join(source_frame_dir, f))]):\n",
    "    if \"source_graph_descriptor.data\" in graph:\n",
    "        continue\n",
    "    print(graph)\n",
    "    timeframe_ds = RedditUserDataset.load_from_instance_file(graph)\n",
    "    timeframe_ds.shorten_similarity_triplet_list(threshold)\n",
    "    timeframed_dataset.append(timeframe_ds)\n",
    "    doc_sum = 0\n",
    "    users = 0\n",
    "    for index, row in RedditUserDataset.load_from_instance_file(graph).data_frame.iterrows():\n",
    "        users += 1\n",
    "        doc_sum += row['num_docs']\n",
    "    doc_amount_avgs.append(doc_sum / users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b213a9-9116-4782-8d04-a11717007f90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1887cea-7589-488f-a326-26c58e4df396",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids='../data/reddit_dataset/user_splits/binary_unbalanced'\n",
    "\n",
    "# Split user\n",
    "if do_user_split:\n",
    "    train_ids = read_userids('train_ids.txt', user_ids)\n",
    "    val_ids = read_userids('val_ids.txt', user_ids)\n",
    "    test_ids = read_userids('test_ids.txt', user_ids)\n",
    "\n",
    "    # Split validation\n",
    "    for uid in train_ids:\n",
    "        if uid in val_ids:\n",
    "            raise Exception(\"Invalid split!\")\n",
    "        if uid in test_ids:\n",
    "            raise Exception(\"Invalid split\")\n",
    "\n",
    "    for uid in val_ids:\n",
    "        if uid in test_ids:\n",
    "            raise Exception(\"Invalid split!\")\n",
    "\n",
    "    train_sample_frame = base_dataset.filter_user_ids(train_ids, inplace=False).data_frame\n",
    "    print(len(train_sample_frame))\n",
    "    val_sample_frame = base_dataset.filter_user_ids(val_ids, inplace=False).data_frame\n",
    "    print(len(val_sample_frame))\n",
    "    test_sample_frame = base_dataset.filter_user_ids(test_ids, inplace=False).data_frame\n",
    "    print(len(test_sample_frame))\n",
    "else:\n",
    "    print(\"Users are not being split...\")\n",
    "    train_sample_frame = base_dataset.data_frame\n",
    "    val_sample_frame = base_dataset.data_frame\n",
    "    test_sample_frame = base_dataset.data_frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83006627-52b8-4b21-bc8b-9dbf14995085",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.embed_user('f4c297d70c985a246ce50bb9c2554d0e2a7743e1488dcf07bed4b4e990de342a', time_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75e990-db93-476b-8d1c-dbe26e43bca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'f4c297d70c985a246ce50bb9c2554d0e2a7743e1488dcf07bed4b4e990de342a' in list(embedder.users_embeddings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa3bca9-32a4-450f-a202-3b386de1a725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d5fdf-9c63-4580-82b4-b7979a4c45c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "precomputed_features = {}\n",
    "\n",
    "for _, row in base_dataset.data_frame.iterrows():\n",
    "    precomputed_features[row['user_id']] = []\n",
    "\n",
    "random_features = False\n",
    "embed_mode = 'avg'\n",
    "if not random_features:\n",
    "    # TODO: compute features from document embeddings files to timeframed_dataset.\n",
    "    for time_index, frame in enumerate(timeframed_dataset):\n",
    "        print('Precomputing features for timeframe...')\n",
    "        feature_map = frame.compute_features(embedder, time_index=time_index, embed_mode=embed_mode)\n",
    "        for index, feature in feature_map.items():\n",
    "            if index in precomputed_features.keys():\n",
    "                precomputed_features[index].append(feature_map[index])\n",
    "else:\n",
    "    for frame in timeframed_dataset:\n",
    "        print('Precomputing random features...')\n",
    "        for _, row in frame.data_frame.iterrows():\n",
    "            precomputed_features[row['user_id']].append(torch.tensor(np.random.uniform(low=-1.5, high=1.5, size=dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69f6c89-5b2b-4195-8896-379861fc95c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a9c720-0118-4bb7-b847-75bf749ec562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4f4327-af4e-425a-b3ad-b32bd87deee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_counter = -1\n",
    "n_train_samples=1000\n",
    "\n",
    "n_users=200 \n",
    "sampling_seeds = [int(random.uniform(0, 1000000)) for i in range(n_train_samples + 1 + 200)]\n",
    "train_min_index=0 \n",
    "train_max_index=1\n",
    "start = time.time()\n",
    "for n in range(n_train_samples):\n",
    "    seed_counter += 1\n",
    "    sample_ids = train_sample_frame.sample(n=n_users, random_state=sampling_seeds[seed_counter])['user_id']\n",
    "    sample_frames = [frame.filter_user_ids(sample_ids, inplace=False) for frame in timeframed_dataset]\n",
    "\n",
    "    start_frame = train_min_index\n",
    "    train_window = train_max_index - train_min_index\n",
    "    print(start_frame)\n",
    "    print(start_frame + train_window)\n",
    "    sample_frames = sample_frames[start_frame:start_frame + train_window]\n",
    "    \n",
    "    if False:\n",
    "        sample_frames = [\n",
    "            tf.build_graph_column_precomputed(threshold=threshold, percentage=percentage, inplace=False).data_frame for\n",
    "            index, tf in\n",
    "            enumerate(sample_frames)]\n",
    "    else:\n",
    "        sample_frames = [\n",
    "            tf.data_frame for tf in sample_frames\n",
    "        ]\n",
    "    \n",
    "    sample = convert_timeframes_to_model_input(sample_frames, {k: v[start_frame:start_frame + train_window] for k, v in\n",
    "                                                               precomputed_features.items()}, ground_truth, dim)\n",
    "    sample.print_shapes()\n",
    "    pkl.dump(sample, gzip.open(os.path.join(target_dir, 'train_samples/') + 'sample_' + str(n) + '.data', 'wb'))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Elapsed time:\" + str(end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81926e3-a7d4-4016-b862-87be2d26445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "panda_lst = sample_frames\n",
    "precomputed_features = {k: v[start_frame:start_frame + train_window] for k, v in\n",
    "                                                               precomputed_features.items()}\n",
    "num_features = dim\n",
    "user_index = {}\n",
    "counter = 0\n",
    "for _, row in panda_lst[0].iterrows():\n",
    "    user_index[row['user_id']] = counter\n",
    "    counter += 1\n",
    "\n",
    "num_users = len(user_index.values())\n",
    "graph_data = []\n",
    "feature_data = []\n",
    "labels = torch.zeros(num_users).type(torch.LongTensor)\n",
    "\n",
    "# Generate graph data\n",
    "print(\"Generating graphs...\")\n",
    "for frame in panda_lst:\n",
    "    edges_out = []\n",
    "    edges_in = []\n",
    "    for _, row in frame.iterrows():\n",
    "        out_nodes_map = row['social_graph']\n",
    "        for edge, weight_map in out_nodes_map.items():\n",
    "            if edge in user_index.keys():\n",
    "                edges_out.append(user_index[row['user_id']])\n",
    "                edges_in.append(user_index[edge])\n",
    "    edges = torch.tensor([edges_out, edges_in]).long()\n",
    "    graph_data.append(edges)\n",
    "\n",
    "# Generate feature data\n",
    "print(\"Generating features...\")\n",
    "for counter, frame in enumerate(panda_lst):\n",
    "    timeframe_features = torch.zeros([num_users, num_features])\n",
    "    for index, row in frame.iterrows():\n",
    "        print(row['user_id'])\n",
    "        print(user_index[row['user_id']])\n",
    "        print(precomputed_features[row['user_id']])\n",
    "        timeframe_features[user_index[row['user_id']]] = precomputed_features[row['user_id']][counter]\n",
    "    feature_data.append(timeframe_features)\n",
    "feature_data = torch.stack(feature_data)\n",
    "# Generate ground labels\n",
    "print(\"Generating ground truth...\")\n",
    "for _, row in panda_lst[0].iterrows():\n",
    "    labels[user_index[row['user_id']]] = int(ground_truth[row['user_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1292e9a5-a01d-4f36-9b6a-8ac2aa784fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(precomputed_features.keys())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507fa928-2a7c-4967-9bbe-e1c35fb5d0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
